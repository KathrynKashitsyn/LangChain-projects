{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUe7spOjwQImLdyRzXFYpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KathrynKashitsyn/LangChain-projects/blob/main/Chat_models_and_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the follow along for foundational [`tutorial`](https://python.langchain.com/docs/tutorials/llm_chain/)"
      ],
      "metadata": {
        "id": "VWV-tPaICgiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Installation\n",
        "To install LangChain run:"
      ],
      "metadata": {
        "id": "Pqk8R8MpGtos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpcdjjJdGNS-",
        "outputId": "305336bd-7fd4-46cd-81cc-f4e8a69edc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith."
      ],
      "metadata": {
        "id": "WdhLxSttG2iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to use a language model by itself\n",
        "LangChain supports many different language models that you can use interchangeably."
      ],
      "metadata": {
        "id": "Z9MitPv4Hx2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select and paste chat model:\n",
        "\n",
        "```\n",
        "pip install -qU \"langchain[openai]\"\n",
        "```\n",
        "\n",
        "```\n",
        "pip install -qU \"langchain[anthropic]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[google-genai]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[google-vertexai]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[aws]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[groq]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[cohere]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-nvidia-ai-endpoints\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[fireworks]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[mistralai]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain[together]\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-ibm\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"databricks-langchain\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-xai\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-perplexity\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-deepseek\"\n",
        "```\n",
        "```\n",
        "pip install -qU \"langchain-oci\""
      ],
      "metadata": {
        "id": "UUr-NLuwIBbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "paste here:"
      ],
      "metadata": {
        "id": "9SIHDOiZMFKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "id": "eCsmL6rHMEVq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logging traces\n",
        "Set the environment variables to start logging traces.\n",
        "\n",
        "> LLM tracing is the practice of tracking and understanding the step-by-step decision-making and thought processes within LLMs as they generate responses. This is done by collecting information on the requests and their flow throughout the system.\n",
        "\n"
      ],
      "metadata": {
        "id": "rIf5r1DeMLPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the code, get your LangSmith API key [here](https://smith.langchain.com/o/4f0a581f-c4f5-43f9-bc38-12f343cf110e/settings/apikeys)"
      ],
      "metadata": {
        "id": "GYPK_7hv6X41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "try:\n",
        "    # load environment variables from .env file (requires `python-dotenv`)\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\" #LangSmith - a tool that tracks and debugs AI applications\n",
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
        "        prompt=\"Enter your LangSmith API key (optional): \"\n",
        "    )\n",
        "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
        "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
        "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
        "    )\n",
        "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
        "        os.environ[\"LANGSMITH_PROJECT\"] =\"application\" # or any other project name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFsHUd__G3TI",
        "outputId": "523ad3c1-2944-47ae-b054-d31f38724aa7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key (optional): ··········\n",
            "Enter your LangSmith Project Name (default = \"default\"): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI API key acquired [here](https://platform.openai.com/api-keys)"
      ],
      "metadata": {
        "id": "y-pprM6Y2RKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "cm_D9nfp0VNI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Direct use\n",
        "\n",
        " First use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the `.invoke` method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">  The **Runnable** interface is a universal adapter plug for AI components, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\n",
        "\n",
        "\n",
        "\n",
        "> **Messages** are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iJmwrIf6y04n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRCNdfaOH0X8",
        "outputId": "fe93c9e4-6eb6-4b30-cdc6-f33e5b2f46f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-COFW8SGiIvWJNEgWpS1Q3YqlFa3V8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c97e2a0b-afdd-41c5-9148-e8d60c54a694-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts."
      ],
      "metadata": {
        "id": "BiSi69CK9msV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:"
      ],
      "metadata": {
        "id": "VEcvx4wU-AG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Hello\")\n",
        "\n",
        "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
        "\n",
        "model.invoke([HumanMessage(\"Hello\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOC7rTcM3THq",
        "outputId": "f801f6bf-cbb7-4c74-9b78-b43f5c673233"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-COFWLUtuMNyXEuIGiReeEvT7PkVQ9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--49c9e0cc-0f2d-4ba1-aac9-da4e3e39d3e6-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming\n",
        "Because chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\n",
        "\n"
      ],
      "metadata": {
        "id": "eVdQ73wv-jzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUbo5Ru43pc9",
        "outputId": "9d805093-9538-419f-9d11-70073b83fed3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|C|iao|!|||"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates\n",
        "\n",
        "Prompt templates are a concept in LangChain designed to assist with transformation including addition of a system message or formatting a template with the user input. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
        "\n",
        "Let's create a prompt template here. It will take in two user variables:\n",
        "\n",
        "`language:` The language to translate text into\n",
        "\n",
        "`text:` The text to translate"
      ],
      "metadata": {
        "id": "7rpNnV9n-s8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "x64B-z0-3p-L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ChatPromptTemplate` supports multiple message roles in a single template. We format the `language` parameter into the system message, and the user `text` into a user message.\n",
        "\n",
        "The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself"
      ],
      "metadata": {
        "id": "iTOBbkwv_u61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGhnbXy4ADBp",
        "outputId": "6450f523-7ec7-401a-9093-34a4927a44d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:"
      ],
      "metadata": {
        "id": "Soz_NU2PALy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q13eAIChAMTo",
        "outputId": "498349e0-8cd2-46f8-e816-902ffbdb3d11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can invoke the chat model on the formatted prompt:"
      ],
      "metadata": {
        "id": "jQVje_IbAPt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnMMvyvNAQHE",
        "outputId": "b764274c-bb1a-4ca2-818a-1e9bbf5f8a65"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information."
      ],
      "metadata": {
        "id": "32UOiQ4yAWIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recycle the application"
      ],
      "metadata": {
        "id": "gAzS8nEaAeq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some different use case"
      ],
      "metadata": {
        "id": "ai9iSEVzA190"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Japanese 'bye!' translation and pronunciation"
      ],
      "metadata": {
        "id": "eApiEimHBm-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Japanese and write how it is pronounced\"),\n",
        "    HumanMessage(content=\"bye!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fhSa3LOAWkI",
        "outputId": "9330c9aa-bf8a-42d8-ea5b-0002454e98da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='さようなら！ (Sayōnara!)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-COFr5hTkfpjmPAAD0hZTugofpZZTk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4678daa9-31e2-44fa-9377-2d21cceb80c0-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token count"
      ],
      "metadata": {
        "id": "KceTZIuPB7_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5YdxkbYA0h2",
        "outputId": "8466a40d-8566-47ea-d9db-519ea844bcfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|さ|よう|なら|！| (|Say|ō|n|ara|!)|||"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language} and write pronunciation\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "oHWg71TXA_Zk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Russian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt.to_messages()\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6tURv4iBP_k",
        "outputId": "0abf85bd-a6ca-4915-e735-c7353dbd5bc1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Привет! (Privet!)\n"
          ]
        }
      ]
    }
  ]
}